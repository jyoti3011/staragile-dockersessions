Configuration Management with Ansible:
====================================

Configuration:

Making changes to your environment like:
 - creating files, directories
 - copying files form system to other 
 - creation of users and giving permissions to them
 - Installation/upgrade/uninstallation of packages
 - Deploying of applications
 - creating Db, tables

In an organization there can be many environment (set of VMs) like dev, Qa, prod

Configuration changes can be done manually or automatically - Configuration Management

If we have to do changes on all environments manually :
	- time consuming
	- repetitive
	- Boring
	- prone human errors
	- effort and expensive

When working in DevOps we want to deliver/deploy an application at a faster rate 
When working in DevOps we have to avoid any manual task
When working in DevOps Configuration management on the servers should also be automated
This can be automated using CM tools 
- Ansible
- puppet
- chef
- saltstack

Ansible:
===================

Ansible is a CM tool
It is an open source tool
It works on push approach-> ansible pushes the configuration changes on the worker machines
Ansible is product of RedHat, now its taken over by IBM
Ansible will always be installed on a Linux machine
Ansible will connect to Linux/windows machine for doing configuration changes
Ansible is very easy to learn tool
Ansible has 2 parts: Ansible Core and Ansible tower
Ansible Core : it is the command line tool where we run ansible commands
Ansible tower: it is the GUI of ansible -> it is available with ansible enterprise version, however a trial version is available for use
We will be learning Ansible Core
Ansible code is written in YAML - and the file is called as playbook

Ansible Components:
====================

1. Inventory: WHERE TO DO THE CONFIGURATION CHANGES
====================
- it is a simple text file
- it is present by default in the directory /etc/ansible
- Name of the file by default is hosts
- you can give any other name to the file
- In this file we will write Ip address or hostnames of the servers that ansible has to connect to do changes

2. MODULES: What CHANGES TO DO
===========================
Modules are small python programs pre-written and provided to you by Ansible
There are 4000+ modules
for any configuration tasks that we want to automate, ansible gives a modules 
To this module you give some input values
The module is executed on the worker machine and the required configuration changes are present on worker machine
Each module will do a specific task based on the input values that you provide

Example:
==================

command module: this module will execute any command on your worker machine

ansible --> command(echo "hello All") => 

copy module -. the module has the logic to copy files form source to destination

ansible --> copy(src/file, dest/file)



java:

method:

add(a,b)
{
   result = a+b;
   return(result);

}


file.java
{

method.add(100,20)  ==> 120


}

3. Playbooks:
==========================

Anisble code is written in playbooks
Playbook are written in YAML
A playbook consists of 2 things:

hosts:  
  workers machine that we have to connect, these worker machine IP address should be in inventory file
tasks:
   modules to be executed on the worker machine 


When the playbook is executed 
playbook --> is converted to a python program --> this python file is copied using SSH on the desired worker machine
This python code is then executed by ansible on the worker machine -->
After python file has been executed , it is deleted from the worker machine
All the details can be seen on the ansible controller machine.

4. ansible.cfg

This is ansible configuration file
The name of the file will always be ansible.cfg
The file is present by default in /etc/ansible
You can create the file in any location
In this file we have complete details about ansible
- location of modules
- location on inventory
- user details
- forks
- ansible plugin details


Ansible Installation:
=====================

# sudo su -
$ sudo apt update
$ sudo apt install software-properties-common
$ sudo add-apt-repository --yes --update ppa:ansible/ansible
$ sudo apt install ansible

On the MAster node -> generate the SSH keys
# ssh-keygen

Copy the public ssh key

# cat /root/.ssh/id_rsa.pub


GO TO the worker NODE now

# sudo su -

# echo "<your public key>" >> authorized_keys 

Now go back to Master and update the inventory file

# cd /etc/ansible

# vim hosts


[webserver]
<privateIp of worker>


save the file

run ansible adhoc commands

# ansible weberver -m ping


















Demo:
====================
Ansible code is written in 2 ways:

1. Ansible Ad hoc command

single line command
only 1 module will be executed
only 1 task will be performed on worker
it should be used when we want to quickly check something on the worker node

Syntax :
===========

ansible [hostgroup] -m [modulename] -a [argument/input]

===============================================================
Playbook:

# vim playbook1.yml

---
- name: A playbook to print a message
  hosts: webserver
  tasks:
  - name: Print a message
    debug: msg="Hello from Ansible Controller"
  - name: Execute a command
    command: hostname -s


# ansible-playbook playbook1.yml

# vim playbook2.yml

---
- name: Store the output of a task and print the output
  hosts: webserver
  tasks:
  - name: Exeucte a command
    command: hostname -s
    register: output_command   # it is a variable registering the output of command module
  - name: Print the variable value
    debug: var=output_command

# ansible-playbook playbook2.yml


# vim playbook3.yml

---
- name: Install packages on worker
  hosts: webserver
  tasks:
  - name: Update apt repo
    command: apt-get update
  - name: Install apache2 package
    package: name=apache2 state=present
  - name: Start apache2 service
    service: name=apache2 state=started

# ansible-playbook playbook3.yml

 # vim playbook4.yml

---
- name: Install multiple packages
  hosts: webserver
  tasks:
  - name: Install mutliple package
    package: name={{ item }}  state=present
    loop:
     - php
     - nano
     - git
     - docker.io
     - maven

# ansible-playbook playbook4.yml


# vim htmldeployment.yml

---
- name: Deploy HTML code on Apche2 server
  hosts: webserver
  tasks:
  - name: Print the start of deployment
    debug: msg="Deployment started..."
  - name: Install package
    package: name=apache2 state=present
  - name: Check apache is installed or not
    command: apache2 -v
    register: output_var
  - name: Print the version of apache2
    debug: var=output_var
  - name: Copy HTML code on apache2 server
    copy: src=index.html dest=/var/www/html
  - name: Print status of code file
    debug: msg="index.html file copied successfully .."
  - name: Restart apache2
    service: name=apache2 state=restarted
  - name: Status of Deployment
    debug: msg="Deployment completed .."


# vim index.html

index.html

<h1> This code file is from Ansible Contorller </h1>
<h2> This file is created by Sonal </h2>
<h2> We are learning Ansible as configuration mamanagement tool </h2>


=================================================
Roles:

# mkdir roles

# cd roles

# ansible-galaxy init apache-project

# ls

# vim tasks/main.yml

  - name: Install the package {{pkg_name}}
    package: name={{pkg_name}} state=present
  - name: Start {{pkg_name}} service
    service: name={{pkg_name}} state=started
  - name: Deploy index.html file
    copy: src=index.html dest={{dest_path}}
    notify: Restart {{pkg_name}} service


Save the file

# vim vars/main.yml

pkg_name: apache2
dest_path: /var/www/html
http_port: 80
document_root: /var/www/html

Save the file


# vim handlers/main.yml


- name: Restart {{pkg_name}} service
  service: name={{pkg_name}} state=restarted

Save the file

# vim files/index.html

<marquee> LOOK MY PROJECT EXECUTED SUCCESSFULLY </marquee>
<h1> This is project 1 for the course </h1>
<h1> Created by Sonal Mittal </h1>



Save the file


# cd

# vim playbook-roles-Project.yml

- name: Project 1 of the course
  hosts: webserver
  become: true
  roles:
  - apache-Project



Run the playbook and see the application deployment

For your reference:

NGINX deployment: CEP1
====================

playbookNginx.yml

- name: Deploy on nginx
  hosts: webserver
  become: true
  vars:
   http_port: 80
   document_root: /var/www/html
  tasks:
  - name: Install nginx
    package: name=nginx state=present
  - name: Start Nginx
    service: name=nginx state=started
  - name: update nginx conf
    template: src=default.j2 dest=/etc/nginx/sites-available/default
    notify: Restart nginx service
  - name: Copy the index.html file
    copy: src=index.html dest=/var/www/html
  handlers:
  - name: Restart nginx service
    service: name=nginx state=restarted


# vim index.html

<h1> Nginx deployment </h1>


===============================================

> Ansible Dynamic Inventory

By default in ansible we have a hosts file where we write the list of Ip address of the hosts servers where we have to do the changes.
This file is static where the ip address are fixed, user will manually add new ips or remove Ipaddress that are not required.

But consider a use case where your infrastructure on the cloud and the number of servers are scaling up or scaling down dynamically.

Since the inventory on AWS is dynamic, we cannot hard code the inventory/hosts file on Ansible controller. That will not be correct approach

What is the solution then?

We will take help of Ansible where
Ansible will connect to AWS securely
We will use an ansible plugin that will check number of VMs in the desired region on AWS and fetch the ip address or hostnames on the ansible controller machine.
Once ansible collects the ipaddress of the VMs on the cloud it will automatically compute an Inventory file for the user
We will use a command to generate this inventory file
Whenever the command is run  Ansible → connect to AWS→ go to desired region → collects IP of available VMs→ displays the inventory file on the controller
Now whenever the user will run the playbook, ansible will use the dynamic inventory and execute the changes on the dynamic VMs

Steps for 1 st part:
===================================
First Prepare Ansible Controller to install packages that required to connect to AWS
Install the ansible Cloud plugin and update its details in ansible.cfg file
Create a new inventory file ->The name of the inventory file should always end with aws_ec2.yml

Steps of Part 2:

1. Create credentials on AWS, so ansible can connect to AWS
2. Create some Ec2 Vms on AWS of which ansible will fetch the inventory details.

Steps of Part3:

Ansible Controller connects to those EC2 servers using SSH
Ansible controller executes playbook on the server


Note:

The SDK is composed of two key Python packages: Botocore (the library providing the low-level functionality shared between the Python SDK and the AWS CLI) and Boto3 (the package implementing the Python SDK itself).

https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html

Execute below steps:
=============================

Whichever user you are logged in with the same user you will install the packages and run the ansible inventory command

# su - ansiuser

# cd /etc/ansible

Install ansible aws_ec2 plugins
The plugin is part of the amazon.aws collection
We will install the desired collection

# ansible-galaxy collection install amazon.aws

In order to install boto3 and botocore we need python3-pip package

# sudo apt install python3-pip

# pip3 install boto3

# pip show boto3

# sudo apt-get update

# sudo apt-get install awscli -y

Update the ansible.cfg file for it to use the aws_ec2 plugin to fetch the inventory

# vim ansible.cfg

[defaults]


enable_plugins = aws_ec2

Save the file

Create the aws_ec2 inventory file

# vim aws_ec2.yml

plugin: amazon.aws.aws_ec2
regions:
 - us-east-1


Save the file





Go to AWS  AND pick up the accesskey credentials:






On the Ansible Controller:

Take the access key and secret key and save it as environment variables on the Lab:

Run these commands

 # export AWS_ACCESS_KEY_ID=
#export AWS_SECRET_ACCESS_KEY=

Now execute the command to list the inventory:

# ansible-inventory -i /home/ansiuser/aws_ec2.yml --list





Create Ec2 instances on AWS 

On the master node run the inventory command:

# ansible-inventory -i /home/ansiuser/aws_ec2.yml --list




Do these steps as Assignment:
============================================

Connecting Ansible controller to your cloud machine over SSH
===========================================


Run the Ansible command on the dynamic inventory

If we want to create 10 servers, we would not want to execute steps of creating ansiuser, copying ssh keys and updating config file manually on all server
Solution for this is : 
Take the created ANsible worker to aws that connected to Ansible controller via ssh
Convert the node as an AMI
So all the worker nodes configuration/softwares/OS/user details will be available as AMI
Now we will create new Vms with our custom AMI -> So all the new instances will have
  Ansible controller SSH keys
 Will have ansiuser and config file updates

Assignment/homework steps: 
=======================================
Connect to the Lab
Go to AWS  , recreate the user and accesskey and secret key
Reset the below variables on the Master node
export AWS_ACCESS_KEY_ID=
 export AWS_SECRET_ACCESS_KEY=

Now execute the command to list the inventory:

# ansible-inventory -i aws_ec2.yml --list

Connect to the AWS Ec2 instance and perform the 4 steps for ssh communication


On the master node copy the ssh keys on the new ec2 machine
# ssh-copy-id -i ansiuser@publicipEC2
# ssh-copy-id -i ansiuser@54.163.18.51
Now go to AWS and select your EC2 server
Convert the Ec2 server in to an Image


Give the Image name as AnsibleHostImage→ click on create Image

You can see you image by clicking on AMI



It will take 5 mins to create the AMI
Once the AMi is available.
7. We will now create multiple Instance with the new Image itself
Click on launch Instance → neter the name and selct the AMi that we have created


Launch the instance
8 go to the master node:
# ansible-inventory -i aws_ec2.yml --list
# ansible aws_ec2 -i aws_ec2.yml -m ping
================================================


























